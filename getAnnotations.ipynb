{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_response():\n",
    "    results = []\n",
    "    print ('\\n')\n",
    "    response = input('ENTER sentence number: ')\n",
    "    results.append( int(response) )\n",
    "    responces = 1\n",
    "    while int(response) != 0 and responces < 5:\n",
    "        response  = int (input('ENTER sentence number: '))\n",
    "        if response in results:\n",
    "            print (\"Notice: You have already selected sentence number: \" + str(response) + \" please select another sentence\")\n",
    "        else:\n",
    "            results.append( response )\n",
    "            responces = responces + 1\n",
    "    print (\"\\n\")\n",
    "    return 0\n",
    "\n",
    "annotation_data = []\n",
    "with open('temp_annotation_file.txt', 'r') as f:\n",
    "    annotation_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following are most descriptive of future work (if any). Vote at most 5.\n"
     ]
    }
   ],
   "source": [
    "positive_labels = []\n",
    "negative_labels = []\n",
    "paperIds2Sents = {}\n",
    "print( \"Which of the following are most descriptive of future work (if any). Vote at most 5.\" )\n",
    "for line in annotation_data:\n",
    "    item = line.split('\\t')\n",
    "    paperID = item[0]\n",
    "    title = item[1]\n",
    "    query = item[2]\n",
    "    if paperID not in paperIds2Sents:\n",
    "        paperIds2Sents[paperID] = {}\n",
    "        paperIds2Sents[paperID]['sents'] = []\n",
    "    \n",
    "    paperIds2Sents[paperID]['title'] = title\n",
    "    paperIds2Sents[paperID]['sents'].append(query)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTIONS: You are given the title of the article followed by several sentences. Select the most relevent to future works.\n",
      "Practical Issues In Automatic Documentation Generation\n",
      "1: 6 conc lus ion  plandoc will move into actual use in fall 1994\n",
      "\n",
      "2: at this point, we will be able to fully evaluate how well its output meets user needs\n",
      "\n",
      "3: furthermore, we plan to augment he system so that it can produce summaries of both the base plan and the proposed plan\n",
      "\n",
      "4: of these, the proposed plan summary presents somewhat more of a challenge\n",
      "\n",
      "5: it should be about a paragraph in length but succinctly summarize the recommendations made by the planning engineer\n",
      "\n",
      "6: thus, the system must work within tighter space constraints to include all information\n",
      "\n",
      "7: a second problem for this summary is that it must include information from multiple sources\n",
      "\n",
      "8: the proposed plan will include elements of the base plan as well as a subset of the refinements the engineer carried out\n",
      "\n",
      "9: plandoc must determine how to integrate these different pieces of information, with emphasis on the resulting plan and less information on how it was derived\n",
      "\n",
      "10: while we can use some of the same techniques currently used to make the refinements summary both more concise and more fluent (i\n",
      "\n",
      "11: , the combined use of conjunction and paraphrase), more research will be required in discourse planning and selection of textual focus\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/squirtle/local/anaconda3/lib/python3.4/site-packages/IPython/kernel/zmq/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                 \u001b[0mident\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/squirtle/local/anaconda3/lib/python3.4/site-packages/IPython/kernel/zmq/session.py\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m             \u001b[0mmsg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/squirtle/local/anaconda3/lib/python3.4/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[1;34m(self, flags, copy, track)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[1;31m# have first part already, only loop while more to receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:5772)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:5572)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:1725)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/squirtle/local/anaconda3/lib/python3.4/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:6022)\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcdef\u001b[0m \u001b[0mint\u001b[0m \u001b[0merrno\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzmq_errno\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mPyErr_CheckSignals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-59dfd2b82171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaperIds2Sents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpaperID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpositive_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-7e85fe690ec4>\u001b[0m in \u001b[0;36mget_response\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ENTER sentence number: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mresponces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/squirtle/local/anaconda3/lib/python3.4/site-packages/IPython/kernel/zmq/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/squirtle/local/anaconda3/lib/python3.4/site-packages/IPython/kernel/zmq/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for paperID in paperIds2Sents:\n",
    "    print (\"INSTRUCTIONS: You are given the title of the article followed by several sentences. Select the most relevent to future works.\")\n",
    "    print (\"TITLE: \" + paperIds2Sents[paperID]['title'] )\n",
    "    for i, query in enumerate(paperIds2Sents[paperID]['sents']):\n",
    "        print ( str(i+1) + \": \" + query)\n",
    "    results = get_response()\n",
    "    \n",
    "    positive_results = []\n",
    "    negative_results = []\n",
    "    for i, query in enumerate(paperIds2Sents[paperID]['sents']):\n",
    "        if i in positive_results:\n",
    "            positive_results.append(query)\n",
    "        else:\n",
    "            negative_results.append(query)\n",
    "    \n",
    "    with open ('s_positive_annotations.txt', 'a+') as f:\n",
    "        f.write( positive_results ) \n",
    "        \n",
    "    with open ('s_negative_annotations.txt', 'a+') as f:\n",
    "        f.write( negative_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_labels = {}\n",
    "negative_labels = {}\n",
    "with open ('s_positive_annotations.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        item = line.split('\\t')\n",
    "        if item[0] not in positive_labels:\n",
    "            positive_labels[item[0]] = []\n",
    "        positive_labels[item[0]].append(item[1])\n",
    "\n",
    "with open ('s_negative_annotations.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        item = line.split('\\t')\n",
    "        if item[0] not in negative_labels:\n",
    "            negative_labels[item[0]] = []\n",
    "        negative_labels[item[0]].append(item[1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A94-1002': ['6 conc lus ion  plandoc will move into actual use in fall 1994. at this point, we will be able to fully evaluate how well its output meets user needs.\\n',\n",
       "  'furthermore, we plan to augment he system so that it can produce summaries of both the base plan and the proposed plan.\\n',\n",
       "  'of these, the proposed plan summary presents somewhat more of a challenge.\\n',\n",
       "  'it should be about a paragraph in length but succinctly summarize the recommendations made by the planning engineer.\\n',\n",
       "  'thus, the system must work within tighter space constraints to include all information.\\n',\n",
       "  'a second problem for this summary is that it must include information from multiple sources.\\n',\n",
       "  'the proposed plan will include elements of the base plan as well as a subset of the refinements the engineer carried out.\\n',\n",
       "  'plandoc must determine how to integrate these different pieces of information, with emphasis on the resulting plan and less information on how it was derived.\\n'],\n",
       " 'A97-1043': ['we have reported an experimental study for extracting key paragraphs based on the degree of context dependency for a given article and showed how our context dependency model can use effectively to extract key paragraphs, each of which belongs to the restricted subject domain.\\n',\n",
       "  'in order to cope with the remaining problems mentioned in section 7 and apply this work to practical use, we will conduct further experiments.\\n'],\n",
       " 'C04-1057': ['in this paper we proposed a formal model for information selection and redundancy avoidance in summarization and question-answering.\\n',\n",
       "  'within this two-dimensional model, summarization and question-answering entail mapping textual units onto conceptual units, and optimizing the selection of a subset of textual units that maximizes the information content of the covered conceptual units.\\n',\n",
       "  'the formalization of the process allows us to benefit from theoretical results, including suitable approximation algorithms.\\n',\n",
       "  'experiments using duc data showed that this approach does indeed lead to improvements due to better information packing over a straightforward content selection method.\\n'],\n",
       " 'C04-1108': ['in this paper we described our approach to coherent sentence ordering for summarizing newspaper articles.\\n',\n",
       "  'we conducted an experiment of sentence ordering through multi-document summarization.\\n',\n",
       "  'the proposed method which utilizes precedence relation of sentence archived good results, raising poor chronological orderings to an acceptable level by 20%.\\n',\n",
       "  'we also proposed an evaluation metric that measures sentence continuity and a amendment-based evaluation task.\\n',\n",
       "  'the amendment-based evaluation outperformed the evaluation that compares an ordering with an answer made by a human.\\n',\n",
       "  'the sentence continuity metric applied to the amendment-based task showed more agree ments with the rating result.\\n'],\n",
       " 'C82-1013': ['whenever requested, very elementary semantic primitives which allow limited deduction and reasoning capabilities.\\n'],\n",
       " 'C94-1056': ['we have developed an automatic abstract generation system for japanese expository writings based on rhetorical structure extraction.\\n',\n",
       "  'the rhetorical structure provkles a natural order of importance among senteuces in the text, and can be used to determine which sentence should be extracted in the abstract, according to the desired length of the abstract.\\n',\n",
       "  'the rhetorical structure also provkles the rhetorical relation between the extracted sentences, and can be used to generate appropriate connectives between them.\\n',\n",
       "  'abstract generation b~sed on rhetorical structure extraction has four merits.\\n',\n",
       "  'first, unlike conventional word-frequency-based abstract generation systems(e.g.\\n',\n",
       "  '\\\\[kuhn 58\\\\]), the geuerated abstract is consistent with the original text in that the connectives between sentences in the abstract reflect their relation in the original text.\\n',\n",
       "  'second, once the rhetorical structure is obtained, varions lengths of generated abstracts can be generated easily.\\n',\n",
       "  'this can be done by simply repeating the reduction process until one gets the desired length of abstract.\\n',\n",
       "  'third, unlike conventional knowledge or script-b`ased abstr,~t generation systems(e.g.\\n',\n",
       "  '\\\\[lehnert 80\\\\], \\\\[fum 86\\\\]), the rhetorical structure extraction does not need prepared knowledge or scripts related to the original text , aud can be used for texts of any domain , so long as they contain enongh rhetoricm expressions to be expository writings.\\n',\n",
       "  'fourth, the generated abstract is composed of rhetoriealy consistent units which consist of several sentences and form a rhetorical substructure, so the abstract does not contain fragmentary sentences which cannot be understood a lone .\\n',\n",
       "  'the system is now utilized ,as a text browser for a prototypical interactive document retrieval system.\\n'],\n",
       " 'D07-1001': ['in this paper we proposed a novel method for automatic sentence compression.\\n',\n",
       "  'central in our approach is the use of discourse-level information which we argue is an important prerequisite for document (as opposed to sentence) compression.\\n',\n",
       "  'our model uses integer programming for inferring globally optimal compressions in the presence of linguistically motivated constraints.\\n',\n",
       "  'our discourse constraints aim to capture local coherence and are inspired by centering theory and lexical chains.\\n',\n",
       "  'we showed that our model can be successfully employed to produce compressed documents that preserve most of the original?s core content.\\n',\n",
       "  'our approach to document compression differs from most summarisation work in that our summaries are fairly long.\\n',\n",
       "  'however, we believe this is the first step into understanding how compression can help summarisation.\\n',\n",
       "  'the compression rate can be tailored through additional constraints which act on the output length to ensure precise word limits are obeyed.\\n',\n",
       "  'we also plan to study the effect of global discourse structure (daume?\\n',\n",
       "  'iii and marcu 2002) on the compression task.\\n',\n",
       "  'acknowledgements we are grateful to ryan mcdonald for his help with the re-implementation of his system and our annotators vasilis karaiskos and sarah luger.\\n',\n",
       "  'thanks to simone teufel, alex lascarides, sebastian riedel, and bonnie webber for insightful comments and suggestions.\\n',\n",
       "  'lapata acknowledges the support of epsrc (grant gr/t04540/01).\\n'],\n",
       " 'E06-1040': ['corpus quality plays a significant role in automatic evaluation of nlg texts.\\n',\n",
       "  'automatic metrics can be expected to correlate very highly with human judgments only if the reference texts used are of high quality, or rather, can be expected to be judged high quality by the human evaluators.\\n',\n",
       "  'this is especially important when the generated texts are of similar quality to human-written texts.\\n',\n",
       "  'in mt, high-quality texts vary less than generally in nlg, so bleu scores against 4 reference translations from reputable sources (as in mt ?05) are a feasible evaluation regime.\\n',\n",
       "  'it seems likely that for automatic evaluation in nlg, a larger number of reference texts than four are needed.\\n',\n",
       "  'in our experiments, we have found nist a more reliable evaluation metric than bleu and in particular rouge which did not seem to offer any advantage over simple string-edit distance.\\n',\n",
       "  'we also found individual experts?\\n',\n",
       "  'judgments are not likely to correlate highly with average expert opinion, in fact less likely than nist scores.\\n',\n",
       "  'this seems to imply that if expert evaluation can only be done with one or two experts, but a high-quality reference corpus is available, then a nist-based evaluation may produce more accurate results than an expert-based evaluation.\\n',\n",
       "  'it seems clear that for automatic corpus-based evaluation to work well, we need high-quality reference texts written by many different authors and large enough to give reasonable coverage of phenomena such as variation for variation?s sake.\\n',\n",
       "  'metrics that do not exclusively reward similarity with reference texts (such as nist) are more likely to correlate well with human judges, but all of the existing metrics that we looked at still penalised generators that do not always choose the most frequent variant.\\n',\n",
       "  'the results we have reported here are for a relatively simple sublanguage and domain, and more empirical research needs to be done on how well different evaluation metrics and methodologies (including different types of human evaluations) correlate with each other.\\n'],\n",
       " 'I05-5012': ['in this paper, we presented an extension to the viterbi algorithm which selects words in the string that are likely result in probable dependency structures.\\n',\n",
       "  'in a preliminary evaluation using precision and recall of dependency relations, we find that it improves grammaticality over a bigram model.\\n'],\n",
       " 'N06-1048': ['pourpre pioneered automatic nugget-based assessment for definition questions, and thus enabled a rapid experimental cycle of system development.\\n',\n",
       "  'nuggeteer improves on that functionality, and critically adds: ?\\n',\n",
       "  'an interpretable score, comparable to official scores, with near-human error rates, ?\\n',\n",
       "  'a reliable confidence interval on the estimated score, ?\\n',\n",
       "  'scoring known responses exactly, ?\\n',\n",
       "  'support for improving the accuracy of the score through additional annotation, and ?\\n',\n",
       "  'a more robust training process we have shown that nuggeteer evaluates the definition and relationship tasks with comparable rank swap rates to pourpre.\\n',\n",
       "  'we explored the effects of stemming, term weighting, n-gram size, stopword removal, and use of system responses for training, all with little effect.\\n',\n",
       "  'we showed that previous methods of selecting a threshold overtrained, and have 7we used a low threshold to make the task mostly correcting and less searching.\\n',\n",
       "  'this is clearly not how assessors should work, but is expedient for developers.\\n',\n",
       "  'question id 1901, response rank 2, response score 0.14 response text: best american classical music bears its stamp: witness aaron copland, whose \"american-sounding\" music was composed by a (the response was a sentence fragment) assigned nugget description: born brooklyn ny 1900 bigram matches: ?american classical?, ?american-sounding music?, ?best american?, ?whose american-sounding?, ?witness aaron?, ?copland whose?, ?stamp witness?, ... response containing the nugget: even the best american classical music bears its stamp:  ny 1900?\\n',\n",
       "  'at a recall score well above that of the background, despite containing none of those words.\\n',\n",
       "  'briefly described a promising way to select finergrained thresholds automatically.\\n',\n",
       "  'our experiences in using judgements of system responses point to the need for a better annotation of nugget content.\\n',\n",
       "  'it is possible to give nuggeteer multiple nugget descriptions for each nugget.\\n',\n",
       "  'nuggeteer currently supports evaluation for the trec definition, ?other?, and relationship tasks, for the aquaint opinion pilot 8, and is under development for the darpa gale task 9.\\n'],\n",
       " 'P00-1041': ['this paper has presented an alternative to extractive summarization: an approach that makes it possible to generate coherent summaries that are shorter than a single sentence and that attempt to conform to a particular style.\\n',\n",
       "  'our approach applies statistical models of the term selection and term ordering processes to produce short summaries, shorter than those reported previously.\\n',\n",
       "  'furthermore, with a slight generalization of the system described here, the summaries need not contain any of the words in the original document, unlike previous statistical summarization systems.\\n',\n",
       "  'given good training corpora, this approach can also be used to generate headlines from a variety of formats: in one case, we experimented with corpora that contained japanese documents and english headlines.\\n',\n",
       "  'this resulted in a working system that could simultaneously translate and summarize japanese documents.8 the performance of the system could be improved by improving either content selection or linearization.\\n',\n",
       "  'this can be through the use of more sophisticated models, such as additional language models that take into account the signed distance between words in the original story to condition 8since our initial corpus was constructed by running a simple lexical translation system over japanese headlines, the results were poor, but we have high hopes that usable summaries may be produced by training over larger corpora.\\n',\n",
       "  'the probability that they should appear separated by some distance in the headline.\\n',\n",
       "  'recently, we have extended the model to generate multi-sentential summaries as well: for instance, given an initial sentence such as ?clinton to meet visit mideast.?\\n',\n",
       "  'and words that are related to nouns (?clinton?\\n',\n",
       "  'and ?mideast?)\\n',\n",
       "  'in the first sentence, the system biases the content selection model to select other nouns that have high mutual information with these nouns.\\n',\n",
       "  'in the example sentence, this generated the subsequent sentence ?us urges israel plan.?\\n',\n",
       "  'this model currently has several problems that we are attempting to address: for instance, the fact that the words co-occur in adjacent sentences in the training set is not sufficient to build coherent adjacent sentences (problems with pronominal references, cue phrases, sequence, etc.\\n',\n",
       "  'abound).\\n',\n",
       "  'furthermore, our initial experiments have suffered from a lack of good training and testing corpora; few of the news stories we have in our corpora contain multi-sentential headlines.\\n',\n",
       "  'we expect to improve both the quality and scope of the summaries produced in future work.\\n'],\n",
       " 'P02-1057': ['acknoledgements  the compressions obtained starting from perfectly derived discourse trees indicate that perfect discourse structures help greatly in improving coherence and grammaticality of generated summaries.\\n',\n",
       "  'it was surprising to see that the summary quality was affected negatively by the use of perfect discourse structures (although not statistically significant).\\n',\n",
       "  'we believe this happened because the text fragments we summarized were extracted from longer documents.\\n',\n",
       "  'it is likely that had the discourse structures been built specifically for these short text snippets, they would have been different.\\n',\n",
       "  'moreover, there was no component designed to handle cohesion; thus it is to be expected that many compressions would contain dangling references.\\n',\n",
       "  'overall, all our systems outperformed both the random baseline and the concat systems, which empirically show that discourse has an important role in document summarization.\\n',\n",
       "  'we performed ?\\n',\n",
       "  'tests on the results and found that on the wall street journal data, the differences in score between the concat and sent systems for grammaticality and coherence were statistically significant at the 95% level, but the difference in score for summary quality was not.\\n',\n",
       "  'for the mitre data, the differences in score between the concat and sent systems for grammaticality and summary quality were statistically significant at the 95% level, but the difference in score for coherence was not.\\n',\n",
       "  'the score differences for grammaticality, coherence, and summary quality between our systems and the baselines were statistically significant at the 95% level.\\n',\n",
       "  'the results in table 2, which can be also assessed by inspecting the compressions in figure 4 show that, in spite of our success, we are still far away from human performance levels.\\n',\n",
       "  'an error that our system makes often is that of dropping complements that cannot be dropped, such as the phrase ?for re-election?, which is the complement of ?is looking?.\\n',\n",
       "  'we are currently experimenting with lexicalized models of syntax that would prevent our compression system from dropping required verb arguments.\\n',\n",
       "  'we also consider methods for scaling up the decoder to handling documents of more realistic length.\\n',\n",
       "  'six human evaluators rated the systems according to three metrics.\\n',\n",
       "  'the first two, presented together to the evaluators, were grammaticality and coherence; the third, presented separately, was summary quality.\\n',\n",
       "  'grammaticality was a judgment of how good the english of the compressions were; coherence included how well the compression flowed (for instance, anaphors lacking an antecedent would lower coherence).\\n',\n",
       "  'summary quality, on the other hand, was a judgment of how well the compression retained the meaning of the original document.\\n',\n",
       "  'each measure was rated on a scale from ?\\n',\n",
       "  '(worst) to ?\\n',\n",
       "  '(best).\\n',\n",
       "  'we can draw several conclusions from the evaluation results shown in table 2 along with average compression rate (cmp, the length of the compressed document divided by the original length).5 first, it is clear that genre influences the results.\\n',\n",
       "  'because the mitre data contained mostly short sentences, the syntax and discourse parsers made fewer errors, which allowed for better compressions to be generated.\\n',\n",
       "  'for the mitre corpus, compressions obtained starting from discourse trees built above the sentence level were better than compressions obtained starting from discourse trees built above the edu level.\\n',\n",
       "  'for the wsj corpus, compression obtained starting from discourse trees built above the sentence level were more grammatical, but less coherent than compressions obtained starting from discourse trees built above the edu level.\\n',\n",
       "  'choosing the manner in which the discourse and syntactic representations of texts are mixed should be influenced by the genre of the texts one is interested to compress.\\n',\n",
       "  '5we did not run the system on the mitre data with perfect discourse trees because we did not have hand-built discourse trees for this corpus.\\n',\n",
       "  'for testing, we began with two sets of data.\\n',\n",
       "  'the first set is drawn from the wall street journal (wsj) portion of the penn treebank and consists of ??\\n',\n",
       "  'documents, each containing between ???\\n',\n",
       "  'and ???\\n',\n",
       "  'words.\\n',\n",
       "  'the second set is drawn from a collection of stu3this tends to be the case for very short documents, as the compressions never get sufficiently long for the length normalization to have an effect.\\n',\n",
       "  'dent compositions and consists of ?\\n',\n",
       "  'documents, each containing between ?.?\\n',\n",
       "  'and ???\\n',\n",
       "  'words.\\n',\n",
       "  'we call this set the mitre corpus (hirschman et al, 1999).\\n',\n",
       "  'we would liked to have run evaluations on longer documents.\\n',\n",
       "  'unfortunately, the forests generated even for relatively small documents are huge.\\n',\n",
       "  'because there are an exponential number of summaries that can be generated for any given text4, the decoder runs out of memory for longer documents; therefore, we selected shorter subtexts from the original documents.\\n',\n",
       "  'we used both the wsj and mitre data for evaluation because we wanted to see whether the performance of our system varies with text genre.\\n',\n",
       "  'the mitre data consists mostly of short sentences (average document length from mitre is ?\\n',\n",
       "  'sentences), quite in constrast to the typically long sentences in the wall street journal articles (average document length from wsj is ??4???\\n',\n",
       "  'sentences).\\n',\n",
       "  'for purpose of comparison, the mitre data was compressed using five systems: random: drops random words (each word has a 50% chance of being dropped (baseline).\\n',\n",
       "  'hand: hand compressions done by a human.\\n',\n",
       "  'concat: each sentence is compressed individually; the results are concatenated together, using knight & marcu?s (2000) system here for comparison.\\n',\n",
       "  'edu: the system described in this paper.\\n',\n",
       "  'sent: because syntactic parsers tend not to work well parsing just clauses, this system merges together leaves in the discourse tree which are in the same sentence, and then proceeds as described in this paper.\\n',\n",
       "  'the wall street journal data was evaluated on the above five systems as well as two additions.\\n',\n",
       "  'since the correct discourse trees were known for these data, we thought it wise to test the systems using these human-built discourse trees, instead of the automatically derived ones.\\n',\n",
       "  'the additionall two systems were: pd-edu: same as edu except using the perfect discourse trees, available from the rst corpus (carlson et al, 2001).\\n',\n",
       "  '4in theory, a text of ?\\n',\n",
       "  'words has ?6?\\n',\n",
       "  'possible compressions.\\n',\n",
       "  'len log prob best compression ?\\n',\n",
       "  '?c?????\\n',\n",
       "  '?6???6?\\n',\n",
       "  'mayor is now looking which is enough.\\n',\n",
       "  '?c???????4????\\n',\n",
       "  'the mayor is now looking which is already almost enough to win.\\n',\n",
       "  '?c???????\\n',\n",
       "  '?&apos;???&apos;?\\n',\n",
       "  'the mayor is now looking but without support, he is still on shaky ground.\\n',\n",
       "  '?c???6??\\n',\n",
       "  'mayor is now looking but without the support of governer, he is still on shaky ground.\\n',\n",
       "  '?c?8?&apos;??4????6?\\n',\n",
       "  'the mayor is now looking for re-election but without the support of the governer, he is still on shaky ground.\\n'],\n",
       " 'P03-1069': ['in this paper we proposed a data intensive approach to text coherence where constraints on sentence ordering are learned from a corpus of domain-specific 1the summaries as well as the human data are available from http://www.cs.columbia.edu/?noemie/ordering/.\\n',\n",
       "  'so far our evaluation metric measures order similarities or dissimilarities.\\n',\n",
       "  'this enables us to assess the importance of particular feature combinations automatically and to evaluate whether the model and the search algorithm generate potentially acceptable orders without having to run comprehension experiments each time.\\n',\n",
       "  'such experiments however are crucial for determining how coherent the generated texts are and whether they convey the same semantic content as the originally authored texts.\\n',\n",
       "  'texts.\\n',\n",
       "  'we experimented with different feature encodings and showed that lexical and syntactic information is important for the ordering task.\\n',\n",
       "  'our results indicate that the model can successfully generate orders for texts taken from the corpus on which it is trained.\\n',\n",
       "  'the model also compares favorably with human performance on a singleand multiple document ordering task.\\n',\n",
       "  'our model operates on the surface level rather than the logical form and is therefore suitable for text-to-text generation systems; it acquires ordering constraints automatically, and can be easily ported to different domains and text genres.\\n',\n",
       "  'we proposed kendall?s ?\\n',\n",
       "  'as an automated method for evaluating the generated orders.\\n',\n",
       "  'there are a number of issues that must be addressed in future work.\\n',\n",
       "  'for multidocument summarization comparisons between our model and alternative ordering strategies are important if we want to pursue this approach further.\\n',\n",
       "  'several improvements can take place with respect to the model.\\n',\n",
       "  'an obvious question is whether a trigram model performs better than the model presented here.\\n',\n",
       "  'the greedy algorithm implements a search procedure with a beam of width one.\\n'],\n",
       " 'P04-1049': ['the goal of this paper was to evaluate the results of three different kinds of sentence ranking algorithms and one commercially available summarizer.\\n',\n",
       "  'in order to evaluate the algorithms, we compared their sentence rankings to human sentence rankings of fifteen texts of varying length from the wall street journal.\\n',\n",
       "  'our results indicated that a simple paragraphbased algorithm that was intended as a baseline performed very poorly, and that word-based and some coherence-based algorithms showed the best performance.\\n',\n",
       "  'the only commercially available summarizer that we tested, the msword summarizer, showed worse performance than most other algorithms.\\n',\n",
       "  'furthermore, we found that a coherence-based algorithm that uses pagerank and takes non-tree coherence graphs as input performed better than most versions of a coherence-based algorithm that operates on coherence trees.\\n',\n",
       "  'when data from experiments 1 and 2 were collapsed, the pagerank algorithm performed significantly better than all other algorithms, except the coherence-based algorithm that uses in-degrees of nodes in non-tree coherence graphs.\\n'],\n",
       " 'W00-1009': ['we are in the process of performing a user study to collect interagreement data among judges who are asked to label cross-document rhetorical relations.\\n',\n",
       "  'we are also currently building a system for automatic identification of relationships in document clusters as well as a library of summarization perators.\\n',\n",
       "  'user preferrenees are used to constrain the summarizers.\\n',\n",
       "  'for example, a user may prefer that in the event of same set of input documents contradiction, both sources of information should be represented in the summary.\\n',\n",
       "  'another user may have preferences for a given source over all others and choose an operator which will only reflect his preferred source.\\n',\n",
       "  'we introduced a theory of cross-document structure based on inter-document relationships uch as paraphrase, citation, attribution, modality, and development.\\n',\n",
       "  'we presented a taxonomy of cross-document links.\\n',\n",
       "  'we argued that a cst-based analysis of related documents can facilitate multidocument summarization.\\n'],\n",
       " 'W01-0802': ['our experience in three domains shows that human experts build qualitative overviews when writing texts, and that these overviews are used by the experts for inference and to provide a context for specific content rules.\\n'],\n",
       " 'W02-0401': ['we showed how maximum entropy could be used for sentence extraction, and in particular, that adding a prior could deal with the categorical nature of the features.\\n',\n",
       "  'maximum entropy, with an optimised prior, did yield marginally better results than naive bayes (with and without a similarly optimised prior).\\n',\n",
       "  'however, the dierences were not that great.\\n',\n",
       "  'our further experiments with informative features showed that this lack of dierence was probably due (at least in part) to the actual features used, and not due to the technique itself.\\n',\n",
       "  'our oracle results are an idealisation.\\n',\n",
       "  'a fuller comparison should use more sophisticated features, along with more data.\\n',\n",
       "  'our approach treated sentences largely independently of each other.\\n',\n",
       "  'however, abstract-worthy sentences tend to bunch together, particularly at the beginning and end of a document.\\n',\n",
       "  'we intend capturing this idea by making our approach sequencebased: future decisions should also be conditioned on previous choices.\\n',\n",
       "  'a problem with supervised approaches (such as ours) is that we need annotated material (marcu, 1999).\\n',\n",
       "  'this is costly to produce.\\n',\n",
       "  'note that there is a close connection between multi-document summarisation (where many alternative documents all consider similar issues) and the concept of a view in cotraining.\\n',\n",
       "  'in summary, maximum entropy can be benecially used in sentence extraction.\\n',\n",
       "  'however, one needs to guard against categorial features.\\n',\n",
       "  'an optimised prior can provide such help.\\n'],\n",
       " 'W03-0501': ['we have shown the effectiveness of constructing headlines by selecting words in order from a newspaper story.\\n',\n",
       "  'the practice of selecting words from the early part of the document has been justified by analyzing the behavior of humans doing the task, and by automatic evaluation of a system operating on a similar principle.\\n',\n",
       "  'we have compared two systems that use this basic technique, one taking a statistical approach and the other a linguistic approach.\\n',\n",
       "  'the results of the linguistically motivated approach show that we can build a working system with minimal linguistic knowledge and circumvent the need for large amounts of training data.\\n',\n",
       "  'we should be able to quickly produce a comparable system for other languages, especially in light of current multi-lingual initiatives that include automatic parser induction for new languages, e.g.\\n',\n",
       "  'the tides initiative.\\n',\n",
       "  'hedge trimmer will be installed in a translingual detection system for enhanced display of document surrogates for cross-language question answering.\\n',\n",
       "  'this system will be evaluated in upcoming iclef conferences.\\n'],\n",
       " 'W03-1202': ['combining both the svd probability and conditional probability marginally improves recall, lending support to the intuition that thematic information may help generate better single sentence summaries.\\n',\n",
       "  'however, there are still many unanswered questions.\\n'],\n",
       " 'W04-2322': ['future work the u-ldm discussed in this paper represents a significant advance in the theoretical understanding of the nature of discourse structure.\\n',\n",
       "  'the explicit rules for discourse segmentation based on the syntactic reflexes of semantic structures allow analysts for the first time to relate the semantics underlying the syntactic structure of sentences to the discourse segments needed to account for continuity.\\n',\n",
       "  'in addition, the rules for discourse attachment for the first time make clear the principles of discourse continuity for ?coherent?\\n',\n",
       "  'discourse.\\n',\n",
       "  'while full implementation of the principles of discourse organization outlined here are beyond the state of the art in some respects (i.e.\\n',\n",
       "  'determining that a sentence is generic in english is non-trivial in many instances although machine learning techniques might be useful in this regard), we believe that the palsumm system demonstrates the practicality of symbolic discourse parsing using the u-ldm model.\\n',\n",
       "  'the infrastructure for this system has been successfully applied to the task of summarizing documents without a complex semantic component, extensive world knowledge and inference or a subjectively annotated corpus.\\n',\n",
       "  'we believe that the u-ldm parsing methods discussed here can be used for all other complex nlp tasks in which symbolic parsing is appropriate, especially rst trees, our basic algorithm is essentially simpler because rst trees are dependency trees over a large set of different link types, whereas ldm trees are constituent trees over effectively two basic node types: subordinations and non-subordinations.\\n',\n",
       "  'those involving high value document collections where precision is critical.\\n'],\n",
       " 'W04-3252': ['in this paper, we introduced textrank ?\\n',\n",
       "  'a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.\\n',\n",
       "  'in particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by textrank in these applications is competitive with that of previously proposed state-of-the-art algorithms.\\n',\n",
       "  'an important aspect of textrank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.\\n'],\n",
       " 'W04-3254': ['6 discussion and future work  uations based on 200 different sets of 10 model summaries.\\n',\n",
       "  'of the line).\\n',\n",
       "  'however, an examination of the better scoring machine summaries show that in this particular case, their information content is indeed good.\\n',\n",
       "  'the very low human scores appear to be cases of especially short summaries (including one duc summariser) and/or summaries with a deviating angle on the story.\\n',\n",
       "  'it has been suggested in duc circles that a lower n should suffice.\\n',\n",
       "  'that even a value as high as 10 is insufficient is already indicated by the ranking correlation of only 0.76. it becomes even clearer with figure 7, which mirrors figure 6 but uses n=10.\\n',\n",
       "  'the scores for the summaries vary wildly, which means that ranking is almost random.\\n',\n",
       "  'of course, the suggestion might be made that the system ranking will most likely also be stabilised by scoring summaries for more texts, even with such a low (or even lower) n per text.\\n',\n",
       "  'however, in that case, the measure only yields information at the macro level: it merely gives an ordering between systems.\\n',\n",
       "  'a factoid-based measure with a high n also yields feedback on a micro level: it can show system builders which vital information they are missing and which superfluous information they are including.\\n',\n",
       "  'we expect this feedback only to be reliable at the same order of n at which single-text-based scoring starts to stabilise, i.e.\\n',\n",
       "  'around 20 to 30. as the average ranking correlation between two weighted factoid score rankings based on 20 summaries is 0.91, we could assume that the ranking based on our full set of 20 different summaries should be an accurate ranking.\\n',\n",
       "  'if we compare it to the duc information overlap rankings for this text, we find that the individual rankings for d086, d108 and d110 have correlations with our ranking of 0.50, 0.64 and 0.79. when we average over the three, this goes up to 0.83. in van halteren and teufel (2003), we compared a consensus summary based on the topscoring factoids with unigram scores.\\n',\n",
       "  'for the 50 fortuyn summaries, we calculate the f-measure for the included factoids with regard to the consensus summary.\\n',\n",
       "  'in a similar fashion, we build a consensus unigram list, containing the 103 unigrams that occur in at least 11 summaries, and calculate the f-measure for unigrams.\\n',\n",
       "  'the correlation between those two scores was low (spearman?s ?\\n',\n",
       "  '= 0.45).\\n',\n",
       "  'we concluded from this experiment that unigrams, though much cheaper, are not a viable substitute for factoids.\\n',\n",
       "  'summary rankings on the basis of two different sets of n summaries, for n between 1 and 50. most important in the document and b) measures based on two factoid analyses constructed along the same lines should lead to the same, or at least very similar, ranking of a set of summaries which are evaluated.\\n',\n",
       "  'since our measure rewards inclusion of factoids which are mentioned often and early, demand a) ought to be satisfied by construction.\\n',\n",
       "  'for demand b), some experimentation is in order.\\n',\n",
       "  'for various numbers of summaries n, we take two samples of n summaries from the whole set (allowing repeats so that we can use n larger than the number of available summaries; a statistical method called ?bootstrap?).\\n',\n",
       "  'for each sample in a pair, we use the weighted factoid score with regard to that sample of n summaries to rank the summaries, and then determine the ranking correlation (spearman?s ?)\\n',\n",
       "  'between the two rankings.\\n',\n",
       "  'the summaries that we rank here are the 20 human summaries of the kuwait text, plus 16 machine summaries submitted for duc-2002.\\n',\n",
       "  'figure 5 shows how the ranking correlation increases with n for the kuwait text.\\n',\n",
       "  'its mean value surpasses 0.8 at n=11 and 0.9 at n=19.\\n',\n",
       "  'at n=50, it is 0.98. what this means for the scores of individual summaries is shown in figure 6, which contains a box plot for the scores for each summary as observed in the 200 drawings for n=50.\\n',\n",
       "  'the high ranking correlation and the reasonable stability of the scores shows that our measure fulfills demand b), at least at a high enough n. what could be worrying is the fact that the machine summaries (right of the dotted line) do not seem to be performing significantly worse than the human ones (left  stability the main reason to do factoid analysis is to measure the quality of summaries, including machine summaries.\\n',\n",
       "  'in our previous work, we do this with a consensus summary.\\n',\n",
       "  'previously, the weighting factors we suggested were information content, position in the summaries and frequency.\\n',\n",
       "  'we investigated the latter two.\\n',\n",
       "  'each factoid we find in a summary to be evaluated contributes to the score of the summary, by an amount which reflects the perceived value of the factoid, what we will call the ?weighted factoid score (wfs)?.\\n',\n",
       "  'the main component in this value is frequency, i.e., the number of model summaries in which the factoid is observed.\\n',\n",
       "  'experimentation is not complete, but the adjustments appear to influence the ranking only slightly.\\n',\n",
       "  'the results we present here are those using pure frequency weights.\\n',\n",
       "  'we noted in our earlier paper that a good quality measure should demonstrate at least the following properties: a) it should reward inclusion in a summary of the information deemed 2it should be noted that the estimation in figure 4 improves upon the original estimation in that paper, as the determination of number of factoids for that figure did not consider the splitting factor, but just counted the number of factoids as taken from the inventory at its highest granularity.\\n',\n",
       "  '3this is similar to the relative utility measure introduced by radev and tam (2003), which however operates on sentences rather than factoids.\\n',\n",
       "  'it also corresponds to the pyramid measure proposed by nenkova and passonneau (2004), which also considers an estimation of the maximum value reachable.\\n',\n",
       "  'here, we use no such maximum estimation as our comparisons will all be relative.\\n'],\n",
       " 'W06-1643': ['an order-2 crf with skip-chain dependencies derived from the automatic analysis of participant interaction was shown to outperform linear-chain bns and crfs, despite the incorporation in all cases of the same competitive set of predictors resulting from cross-validated feature selection.\\n',\n",
       "  'compared to an order-0 crf model, the absolute increase in performance is 3.9% (7.5% relative increase), which indicates that it is helpful to use skip-chain sequence models in the summarization task.\\n',\n",
       "  'our best performing system reaches 91.3% of human performance, and scales relatively well on automatic speech recognition output.\\n'],\n",
       " 'W97-0710': ['the authors would hke to thank chrm brew, janet httzeman and two anonymous referees for comments on earher drafts of the paper the first author m supported by an epsrc studentshp\\n']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
